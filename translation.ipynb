{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install translate-toolkit transformers sentencepiece\n",
   "metadata": {
    "id": "m063S3oXB5nN",
    "outputId": "2c517e89-d16c-4966-d5c9-a064d1bb8364",
    "execution": {
     "iopub.status.busy": "2022-07-15T08:29:21.269667Z",
     "iopub.execute_input": "2022-07-15T08:29:21.270599Z",
     "iopub.status.idle": "2022-07-15T08:29:36.908625Z",
     "shell.execute_reply.started": "2022-07-15T08:29:21.270562Z",
     "shell.execute_reply": "2022-07-15T08:29:36.907512Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget -O dataset.txt.zip https://opus.nlpl.eu/download.php?f=TEP/v1/moses/en-fa.txt.zip\n",
    "!unzip dataset.txt.zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "from IPython.display import display\n",
    "from IPython.html import widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from transformers import AdamW, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "sns.set()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:100,garbage_collection_threshold:0.8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "with open(\"TEP.en-fa.en\", 'rb') as enfile:\n",
    "  en_lines_list = [line.decode(\"UTF-8\").rstrip() for line in enfile]\n",
    "\n",
    "with open('TEP.en-fa.fa') as fafile:\n",
    "  fa_lines_list = [line.rstrip() for line in fafile]\n",
    "\n",
    "# {\"en\": \"hello\", \"fa\": \"سلام\"}\n",
    "sentences = []\n",
    "for i in range(0, len(en_lines_list)):\n",
    "  sentences.append({\n",
    "        \"en\": en_lines_list[i],\n",
    "        \"fa\": fa_lines_list[i]\n",
    "  })\n",
    "\n",
    "print(\"len en lines: \", len(en_lines_list))\n",
    "print(\"len fa lines: \", len(fa_lines_list))\n",
    "print(\"first en sentence: \", en_lines_list[0])\n",
    "print(\"first fa sentence: \", fa_lines_list[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentences = sentences[:]\n",
    "sentences[2:8]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random.shuffle(sentences)\n",
    "train_slice = int(len(sentences) * 0.80)\n",
    "train_dataset = sentences[:train_slice]\n",
    "test_dataset = sentences[train_slice:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drive.mount('/content/gdrive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare data for feed to model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LANG_TOKEN_MAPPING = {\n",
    "    'en': '<en>',\n",
    "    'fa': '<fa>',\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def encode_input_str(text, target_lang, tokenizer, seq_len,\n",
    "                     lang_token_map=LANG_TOKEN_MAPPING):\n",
    "  target_lang_token = lang_token_map[target_lang]\n",
    "\n",
    "  # Tokenize and add special tokens\n",
    "  input_ids = tokenizer.encode(\n",
    "      text = target_lang_token + text,\n",
    "      return_tensors = 'pt',\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      max_length = seq_len)\n",
    "\n",
    "  return input_ids[0]\n",
    "  \n",
    "def encode_target_str(text, tokenizer, seq_len,\n",
    "                      lang_token_map=LANG_TOKEN_MAPPING):\n",
    "  token_ids = tokenizer.encode(\n",
    "      text = text,\n",
    "      return_tensors = 'pt',\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      max_length = seq_len)\n",
    "  \n",
    "  return token_ids[0]\n",
    "\n",
    "def format_translation_data(translations, input_lang, target_lang,\n",
    "                            tokenizer, seq_len=32):\n",
    "\n",
    "  # Get the translations for the batch\n",
    "  input_text = translations[input_lang]\n",
    "  target_text = translations[target_lang]\n",
    "\n",
    "  if input_text is None or target_text is None:\n",
    "    return None\n",
    "\n",
    "  input_token_ids = encode_input_str(\n",
    "      input_text, target_lang, tokenizer, seq_len, LANG_TOKEN_MAPPING)\n",
    "  \n",
    "  target_token_ids = encode_target_str(\n",
    "      target_text, tokenizer, seq_len, LANG_TOKEN_MAPPING)\n",
    "\n",
    "  return input_token_ids, target_token_ids\n",
    "\n",
    "def transform_batch(batch, input_lang, target_lang, tokenizer):\n",
    "  inputs = []\n",
    "  targets = []\n",
    "  for translation_set in batch:\n",
    "    formatted_data = format_translation_data(\n",
    "        translation_set, input_lang, target_lang, tokenizer, max_seq_len)\n",
    "    \n",
    "    if formatted_data is None:\n",
    "      continue\n",
    "    \n",
    "    input_ids, target_ids = formatted_data\n",
    "    inputs.append(input_ids.unsqueeze(0))\n",
    "    targets.append(target_ids.unsqueeze(0))\n",
    "    \n",
    "  batch_input_ids = torch.cat(inputs).cuda()\n",
    "  batch_target_ids = torch.cat(targets).cuda()\n",
    "\n",
    "  return batch_input_ids, batch_target_ids\n",
    "\n",
    "def get_data_generator(dataset, input_lang, target_lang, tokenizer, batch_size=32):\n",
    "  random.shuffle(dataset)\n",
    "  for i in range(0, len(dataset), batch_size):\n",
    "    raw_batch = dataset[i:i+batch_size]\n",
    "    yield transform_batch(raw_batch, input_lang, target_lang, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Tokenizer & Model English to Persian\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_repo_en2fa = 'persiannlp/mt5-small-parsinlu-translation_en_fa'\n",
    "model_path_en2fa = \"./mt5_translation_en2fa.pt\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer_en2fa = AutoTokenizer.from_pretrained(model_repo_en2fa, use_fast=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model description: https://metatext.io/models/persiannlp-mt5-small-parsinlu-translation_en_fa\n",
    "model_en2fa = AutoModelForSeq2SeqLM.from_pretrained(model_repo_en2fa)\n",
    "model_en2fa = model_en2fa.cuda()\n",
    "model_en2fa.config.max_length = 40\n",
    "max_seq_len = model_en2fa.config.maxlength"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': list(LANG_TOKEN_MAPPING.values())}\n",
    "tokenizer_en2fa.add_special_tokens(special_tokens_dict)\n",
    "model_en2fa.resize_token_embeddings(len(tokenizer_en2fa))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Testing `data_transform`\n",
    "in_ids, out_ids = format_translation_data(\n",
    "    train_dataset[9], \"en\", \"fa\", tokenizer_en2fa)\n",
    "\n",
    "print(' '.join(tokenizer_en2fa.convert_ids_to_tokens(in_ids)))\n",
    "print(' '.join(tokenizer_en2fa.convert_ids_to_tokens(out_ids)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_en2fa.load_state_dict(torch.load(model_path_en2fa))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Constants\n",
    "n_epochs = 2\n",
    "batch_size = 32\n",
    "print_freq = 100\n",
    "checkpoint_freq = 1000\n",
    "lr = 5e-4\n",
    "n_batches = int(np.ceil(len(train_dataset) / batch_size))\n",
    "total_steps = n_epochs * n_batches\n",
    "n_warmup_steps = int(total_steps * 0.01)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model_en2fa.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, n_warmup_steps, total_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def eval_model(model, gdataset, max_iters=8):\n",
    "  test_generator = get_data_generator(gdataset, \"en\", \"fa\",\n",
    "                                      tokenizer_en2fa, batch_size)\n",
    "  eval_losses = []\n",
    "  for i, (input_batch, label_batch) in enumerate(test_generator):\n",
    "    if i >= max_iters:\n",
    "      break\n",
    "\n",
    "    model_out = model.forward(\n",
    "        input_ids = input_batch,\n",
    "        labels = label_batch)\n",
    "    eval_losses.append(model_out.loss.item())\n",
    "\n",
    "  return np.mean(eval_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "losses_en2fa = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_loss = 1000\n",
    "for epoch_idx in range(n_epochs):\n",
    "  # Randomize data order\n",
    "  data_generator = get_data_generator(train_dataset, \"en\", \"fa\",\n",
    "                                      tokenizer_en2fa, batch_size)\n",
    "                \n",
    "  for batch_idx, (input_batch, label_batch) \\\n",
    "      in tqdm_notebook(enumerate(data_generator), total=n_batches):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    model_out = model_en2fa.forward(\n",
    "        input_ids = input_batch,\n",
    "        labels = label_batch)\n",
    "\n",
    "    # Calculate loss and update weights\n",
    "    loss = model_out.loss\n",
    "    losses_en2fa.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print training update info\n",
    "    if (batch_idx + 1) % print_freq == 0:\n",
    "      avg_loss = np.mean(losses_en2fa[-print_freq:])\n",
    "      print('Epoch: {} | Step: {} | Avg. loss: {:.3f} | lr: {}'.format(\n",
    "          epoch_idx+1, batch_idx+1, avg_loss, scheduler.get_last_lr()[0]))\n",
    "      \n",
    "    if (batch_idx + 1) % checkpoint_freq == 0:\n",
    "      test_loss = eval_model(model_en2fa, test_dataset)\n",
    "      print('Test loss: {:.3f}'.format(test_loss))\n",
    "      # dont have overfitting\n",
    "      if abs(test_loss - avg_loss) < 1:\n",
    "        if test_loss < best_loss:\n",
    "          best_loss = test_loss\n",
    "          print('Saving model with test loss of {:.3f}'.format(test_loss))\n",
    "          torch.save(model_en2fa.state_dict(), model_path_en2fa)\n",
    "\n",
    "torch.save(model_en2fa.state_dict(), '/content/gdrive/MyDrive/finalen2fa.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Graph the loss\n",
    "\n",
    "window_size = 50\n",
    "smoothed_losses = []\n",
    "for i in range(len(losses_en2fa)-window_size):\n",
    "  smoothed_losses.append(np.mean(losses_en2fa[i:i+window_size]))\n",
    "\n",
    "plt.plot(smoothed_losses[100:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Load Tokenizer & Model Persian to English"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_repo_fa2en = 'google/mt5-small'\n",
    "model_path_fa2en = \"./mt5_translation_fa2en.pt\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer_fa2en = AutoTokenizer.from_pretrained(model_repo_fa2en, use_fast=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model description: https://huggingface.co/google/mt5-small\n",
    "model_fa2en = AutoModelForSeq2SeqLM.from_pretrained(model_repo_fa2en)\n",
    "model_fa2en = model_fa2en.cuda()\n",
    "model_fa2en.config.max_length = 40\n",
    "max_seq_len = model_fa2en.config.maxlength"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': list(LANG_TOKEN_MAPPING.values())}\n",
    "tokenizer_fa2en.add_special_tokens(special_tokens_dict)\n",
    "model_fa2en.resize_token_embeddings(len(tokenizer_fa2en))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Testing `data_transform`\n",
    "in_ids, out_ids = format_translation_data(\n",
    "    train_dataset[1], \"fa\", \"en\", tokenizer_fa2en)\n",
    "\n",
    "print(' '.join(tokenizer_fa2en.convert_ids_to_tokens(in_ids)))\n",
    "print(' '.join(tokenizer_fa2en.convert_ids_to_tokens(out_ids)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_fa2en.load_state_dict(torch.load(model_path_fa2en))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Constants\n",
    "n_epochs = 3\n",
    "batch_size = 32\n",
    "print_freq = 100\n",
    "checkpoint_freq = 1000\n",
    "lr = 5e-4\n",
    "n_batches = int(np.ceil(len(train_dataset) / batch_size))\n",
    "total_steps = n_epochs * n_batches\n",
    "n_warmup_steps = int(total_steps * 0.01)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model_fa2en.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, n_warmup_steps, total_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def eval_model(model, gdataset, max_iters=8):\n",
    "  test_generator = get_data_generator(gdataset, \"fa\", \"en\",\n",
    "                                      tokenizer_fa2en, batch_size)\n",
    "  eval_losses = []\n",
    "  for i, (input_batch, label_batch) in enumerate(test_generator):\n",
    "    if i >= max_iters:\n",
    "      break\n",
    "\n",
    "    model_out = model.forward(\n",
    "        input_ids = input_batch,\n",
    "        labels = label_batch)\n",
    "    eval_losses.append(model_out.loss.item())\n",
    "\n",
    "  return np.mean(eval_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "losses_fa2en = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_loss = 1000\n",
    "for epoch_idx in range(n_epochs):\n",
    "  # Randomize data order\n",
    "  data_generator = get_data_generator(train_dataset, \"fa\", \"en\",\n",
    "                                      tokenizer_fa2en, batch_size)\n",
    "                \n",
    "  for batch_idx, (input_batch, label_batch) \\\n",
    "      in tqdm_notebook(enumerate(data_generator), total=n_batches):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    model_out = model_fa2en.forward(\n",
    "        input_ids = input_batch,\n",
    "        labels = label_batch)\n",
    "\n",
    "    # Calculate loss and update weights\n",
    "    loss = model_out.loss\n",
    "    losses_fa2en.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print training update info\n",
    "    if (batch_idx + 1) % print_freq == 0:\n",
    "      avg_loss = np.mean(losses_fa2en[-print_freq:])\n",
    "      print('Epoch: {} | Step: {} | Avg. loss: {:.3f} | lr: {}'.format(\n",
    "          epoch_idx+1, batch_idx+1, avg_loss, scheduler.get_last_lr()[0]))\n",
    "      \n",
    "    if (batch_idx + 1) % checkpoint_freq == 0:\n",
    "      test_loss = eval_model(model_fa2en, test_dataset)\n",
    "      print('Test loss: {:.3f}'.format(test_loss))\n",
    "      # dont have overfitting\n",
    "      if abs(test_loss - avg_loss) < 1:\n",
    "        if test_loss < best_loss:\n",
    "          best_loss = test_loss\n",
    "          print('Saving model with test loss of {:.3f}'.format(test_loss))\n",
    "          torch.save(model_fa2en.state_dict(), model_path_fa2en)\n",
    "\n",
    "torch.save(model_fa2en.state_dict(), '/content/gdrive/MyDrive/final_fa2en.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Graph the loss\n",
    "\n",
    "window_size = 50\n",
    "smoothed_losses = []\n",
    "for i in range(len(losses_fa2en)-window_size):\n",
    "  smoothed_losses.append(np.mean(losses_fa2en[i:i+window_size]))\n",
    "\n",
    "plt.plot(smoothed_losses[100:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del model_fa2en\n",
    "del tokenizer_fa2en"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title English To Persian Translation\n",
    "input_text = 'I am new to work as a translator. please ask me simple sentences' #@param {type:\"string\"}\n",
    "output_language = 'fa' #@param [\"fa\"]\n",
    "input_ids = encode_input_str(\n",
    "    text = input_text,\n",
    "    target_lang = output_language,\n",
    "    tokenizer = tokenizer_en2fa,\n",
    "    seq_len = model_en2fa.config.max_length,\n",
    "    lang_token_map = LANG_TOKEN_MAPPING)\n",
    "input_ids = input_ids.unsqueeze(0).cuda()\n",
    "\n",
    "output_tokens = model_en2fa.generate(input_ids, num_beams=20, length_penalty=0.2)\n",
    "print(input_text + '  ->  ' + \\\n",
    "      tokenizer_en2fa.decode(output_tokens[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Persian To English Translation\n",
    "input_text = 'امشب شب خوبی بود' #@param {type:\"string\"}\n",
    "output_language = 'en' #@param [\"en\"]\n",
    "\n",
    "input_ids = encode_input_str(\n",
    "    text = input_text,\n",
    "    target_lang = output_language,\n",
    "    tokenizer = tokenizer_fa2en,\n",
    "    seq_len = model_fa2en.config.max_length,\n",
    "    lang_token_map = LANG_TOKEN_MAPPING)\n",
    "input_ids = input_ids.unsqueeze(0).cuda()\n",
    "print(input_ids)\n",
    "print(tokenizer_fa2en.decode(input_ids[0], skip_special_tokens=True))\n",
    "output_tokens = model_fa2en.generate(input_ids, num_beams=20, length_penalty=0.2)\n",
    "print(input_text + '  ->  ' + \\\n",
    "      tokenizer_fa2en.decode(output_tokens[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!curl bashupload.com -T ./mt5_translation_en2fa.pt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": "#@title English To Persian Translation\ninput_text = 'I am new to work as a translator. please ask me simple sentences' #@param {type:\"string\"}\noutput_language = 'fa' #@param [\"fa\"]\ninput_ids = encode_input_str(\n    text = input_text,\n    target_lang = output_language,\n    tokenizer = tokenizer_en2fa,\n    seq_len = model_en2fa.config.max_length,\n    lang_token_map = LANG_TOKEN_MAPPING)\ninput_ids = input_ids.unsqueeze(0).cuda()\n\noutput_tokens = model_en2fa.generate(input_ids, num_beams=20, length_penalty=0.2)\nprint(input_text + '  ->  ' + \\\n      tokenizer_en2fa.decode(output_tokens[0], skip_special_tokens=True))",
   "metadata": {
    "id": "ERaGbWoXneT6",
    "outputId": "1cd85c5e-6f94-4e8d-98e1-c83540556d46",
    "execution": {
     "iopub.status.busy": "2022-07-15T11:47:59.680806Z",
     "iopub.execute_input": "2022-07-15T11:47:59.681170Z",
     "iopub.status.idle": "2022-07-15T11:48:00.218812Z",
     "shell.execute_reply.started": "2022-07-15T11:47:59.681140Z",
     "shell.execute_reply": "2022-07-15T11:48:00.217605Z"
    },
    "trusted": true
   },
   "execution_count": 81,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title Persian To English Translation\ninput_text = 'امشب شب خوبی بود' #@param {type:\"string\"}\noutput_language = 'en' #@param [\"en\"]\n\ninput_ids = encode_input_str(\n    text = input_text,\n    target_lang = output_language,\n    tokenizer = tokenizer_fa2en,\n    seq_len = model_fa2en.config.max_length,\n    lang_token_map = LANG_TOKEN_MAPPING)\ninput_ids = input_ids.unsqueeze(0).cuda()\nprint(input_ids)\nprint(tokenizer_fa2en.decode(input_ids[0], skip_special_tokens=True))\noutput_tokens = model_fa2en.generate(input_ids, num_beams=20, length_penalty=0.2)\nprint(input_text + '  ->  ' + \\\n      tokenizer_fa2en.decode(output_tokens[0], skip_special_tokens=True))",
   "metadata": {
    "id": "YBmLQizQnfL9",
    "execution": {
     "iopub.status.busy": "2022-07-15T08:05:28.157085Z",
     "iopub.execute_input": "2022-07-15T08:05:28.157711Z",
     "iopub.status.idle": "2022-07-15T08:05:28.419773Z",
     "shell.execute_reply.started": "2022-07-15T08:05:28.157671Z",
     "shell.execute_reply": "2022-07-15T08:05:28.418747Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!curl bashupload.com -T ./mt5_translation_en2fa.pt",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-15T11:25:36.716954Z",
     "iopub.execute_input": "2022-07-15T11:25:36.717308Z",
     "iopub.status.idle": "2022-07-15T11:28:04.296054Z",
     "shell.execute_reply.started": "2022-07-15T11:25:36.717276Z",
     "shell.execute_reply": "2022-07-15T11:28:04.294911Z"
    },
    "trusted": true
   },
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}